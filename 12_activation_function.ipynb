{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed0aed6a",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "\n",
    "Activation function applies non-linear transformation and decide whether a neuron should be active or not.\n",
    "\n",
    "* Without activation functions our networks is basically just a stacked linear regression model.\n",
    "\n",
    "* With non-linear transformation our network can learn better and perform more complex tasks.\n",
    "\n",
    "* After each layer, we typically use an activation function!\n",
    "\n",
    "### Popular Activation functions\n",
    "\n",
    "1. Step function\n",
    "2. Sigmoid\n",
    "3. TanH\n",
    "4. ReLU\n",
    "5. Leaky ReLU\n",
    "6. Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d3a478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
